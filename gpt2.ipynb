{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda3/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda3/envs/llms/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/opt/conda3/envs/llms/lib/python3.10/site-packages/torch/cuda/__init__.py:740: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "from typing import OrderedDict, Optional, List, Tuple, Union\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import math\n",
    "\n",
    "import regex as re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n",
    "from transformers.utils.hub import cached_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"openai-community/gpt2\"\n",
    "\n",
    "# config = GPT2Config.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path)\n",
    "# config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModelOutputWithPastAndCrossAttentions:\n",
    "    last_hidden_state: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CausalLMOutputWithCrossAttentions:\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 50257,\n",
    "        n_positions: int = 1024,\n",
    "        n_embd: int = 768,\n",
    "        n_layer: int = 12,\n",
    "        n_head: int = 12,\n",
    "        n_inner: Optional[int] = None,\n",
    "        activation_function: str = \"gelu_new\",\n",
    "        resid_pdrop: float = 0.1,\n",
    "        embd_pdrop: float = 0.1,\n",
    "        attn_pdrop: float = 0.1,\n",
    "        layer_norm_epsilon: float = 1e-5,\n",
    "        initializer_range: float = 0.02,\n",
    "        summary_type: str = \"cls_index\",\n",
    "        summary_use_proj: bool = True,\n",
    "        summary_activation: Optional[str] = None,\n",
    "        summary_proj_to_labels: bool = True,\n",
    "        summary_first_dropout: bool = 0.1,\n",
    "        scale_attn_weights: bool = True,\n",
    "        use_cache: bool = True,\n",
    "        bos_token_id: int = 50256,\n",
    "        eos_token_id: int = 50256,\n",
    "        scale_attn_by_inverse_layer_idx: bool = False,\n",
    "        reorder_and_upcast_attn: bool = False,\n",
    "        output_attention: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        **kargs,\n",
    "    ) -> None:\n",
    "        # Mapping\n",
    "        self.hidden_size = n_embd\n",
    "        self.max_position_embeddings = n_positions\n",
    "        self.num_attention_heads = n_positions\n",
    "        self.num_hidden_layers = n_layer\n",
    "\n",
    "        # Init\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_positions = n_positions\n",
    "        self.n_embd = n_embd\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_inner = n_inner\n",
    "        self.activation_function = activation_function\n",
    "        self.resid_pdrop = resid_pdrop\n",
    "        self.embd_pdrop = embd_pdrop\n",
    "        self.attn_pdrop = attn_pdrop\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.initializer_range = initializer_range\n",
    "        self.summary_type = summary_type\n",
    "        self.summary_use_proj = summary_use_proj\n",
    "        self.summary_activation = summary_activation\n",
    "        self.summary_proj_to_labels = summary_proj_to_labels\n",
    "        self.summary_first_dropout = summary_first_dropout\n",
    "        self.scale_attn_weights = scale_attn_weights\n",
    "        self.use_cache = use_cache\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx\n",
    "        self.reorder_and_upcast_attn = reorder_and_upcast_attn\n",
    "        self.output_attention = output_attention\n",
    "        self.output_hidden_states = output_hidden_states\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained_model_or_path(pretrained_model_name_or_path: str) -> \"GPT2Config\":\n",
    "        resolved_archive_file = cached_file(\n",
    "            path_or_repo_id=pretrained_model_name_or_path,\n",
    "            filename=CONFIG_NAME,\n",
    "            _raise_exceptions_for_missing_entries=False,\n",
    "        )\n",
    "        \n",
    "        config_content = json.load(open(resolved_archive_file))\n",
    "        return GPT2Config(**config_content)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config.from_pretrained_model_or_path(pretrained_model_name_or_path=pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NewGELUActivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELUActivation(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1D: GPT2 customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(torch.nn.Module):\n",
    "    \"\"\"This is a special customized linear layer, it is used for GPT-2 model.\"\"\"\n",
    "    def __init__(self, input_dim: int, output_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.empty(input_dim, output_dim))\n",
    "        self.bias = torch.nn.Parameter(torch.empty(output_dim))\n",
    "        self.reset_paramter()\n",
    "\n",
    "    def reset_paramter(self):\n",
    "        torch.nn.init.normal_(self.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        # Calculate outout is: (batch_size, seq_len, output_dim)\n",
    "        output_shape = x.size()[:-1] + (self.weight.size(1),)\n",
    "        \n",
    "        # x shape: (batch_size * seq_len, input_dim)\n",
    "        x = x.view(-1, x.size(-1))\n",
    "\n",
    "        # Output shape: (batch_size * seq_len, output_dim)\n",
    "        # x = torch.addmm(self.bias, x, self.weight)\n",
    "        x = F.linear(input=x, weight=self.weight.T, bias=self.bias)\n",
    "        x = x.view(output_shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttention(torch.nn.Module):\n",
    "    def __init__(self, config: GPT2Config) -> torch.Tensor:\n",
    "        super().__init__()\n",
    "        # Init\n",
    "        self.n_head = config.n_head\n",
    "        self.head_size = int(config.hidden_size / self.n_head)\n",
    "        self.scale = 1 / (self.head_size ** 0.5)\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.c_attn = Conv1D(input_dim=config.hidden_size, output_dim=3*config.hidden_size)\n",
    "        self.c_proj = Conv1D(input_dim=config.hidden_size, output_dim=config.hidden_size)\n",
    "        self.attn_dropout = torch.nn.Dropout(p=config.attn_pdrop)\n",
    "        self.resid_dropout = torch.nn.Dropout(p=config.resid_pdrop)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k,\n",
    "        v,\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # QKV\n",
    "        # qkv = self.c_attn(hidden_states)\n",
    "        # q, k, v = qkv.split(self.hidden_size, dim=-1)\n",
    "\n",
    "        # Reshape\n",
    "        q = q.contiguous().view(batch_size, -1, self.n_head, self.head_size).permute(0, 2, 1, 3)\n",
    "        k = k.contiguous().view(batch_size, -1, self.n_head, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.contiguous().view(batch_size, -1, self.n_head, self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past\n",
    "            k = torch.cat((past_key, k), dim=-2)\n",
    "            v = torch.cat((past_value, v), dim=-2)\n",
    "        \n",
    "        if use_cache:\n",
    "            present = (k, v)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        # Compute Q @ K^T\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
    "        attention_scores = attention_scores * self.scale\n",
    "\n",
    "        # Causal mask\n",
    "        seq_len = q.size(-2)\n",
    "        mask_value = torch.finfo(q.dtype).min\n",
    "        causal_mask = torch.triu(torch.full((seq_len, seq_len), mask_value), diagonal=1)\n",
    "        attention_scores = attention_scores + causal_mask\n",
    "\n",
    "        # Attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask[:, None, None, :]\n",
    "            attention_mask = attention_mask.to(dtype=q.dtype)\n",
    "            attention_mask = (1.0 - attention_mask) * torch.finfo(attention_mask.dtype).min\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.attn_dropout(attention_weights)\n",
    "\n",
    "        # Compute V\n",
    "        attention_scores = torch.matmul(attention_weights, v)\n",
    "\n",
    "        # Reshape\n",
    "        context_layer = attention_scores.permute(0, 2, 1, 3).contiguous()\n",
    "        context_layer = context_layer.view(batch_size, -1, self.head_size * self.n_head)\n",
    "        attention_output = self.c_proj(context_layer)\n",
    "\n",
    "        # Skip connection & Dropout\n",
    "        attention_output = self.resid_dropout(attention_output)\n",
    "\n",
    "        outputs = (attention_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attention_weights,)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(torch.nn.Module):\n",
    "    def __init__(self, config: GPT2Config) -> torch.Tensor:\n",
    "        super().__init__()\n",
    "        # Init\n",
    "        self.n_head = config.n_head\n",
    "        self.head_size = int(config.hidden_size / self.n_head)\n",
    "        self.scale = 1 / (self.head_size ** 0.5)\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.c_attn = Conv1D(input_dim=config.hidden_size, output_dim=3*config.hidden_size)\n",
    "        self.c_proj = Conv1D(input_dim=config.hidden_size, output_dim=config.hidden_size)\n",
    "        self.attn_dropout = torch.nn.Dropout(p=config.attn_pdrop)\n",
    "        self.resid_dropout = torch.nn.Dropout(p=config.resid_pdrop)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size = hidden_states.size(0)\n",
    "\n",
    "        # QKV\n",
    "        qkv = self.c_attn(hidden_states)\n",
    "        q, k, v = qkv.split(self.hidden_size, dim=-1)\n",
    "\n",
    "        # Reshape\n",
    "        q = q.contiguous().view(batch_size, -1, self.n_head, self.head_size).permute(0, 2, 1, 3)\n",
    "        k = k.contiguous().view(batch_size, -1, self.n_head, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.contiguous().view(batch_size, -1, self.n_head, self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past\n",
    "            k = torch.cat((past_key, k), dim=-2)\n",
    "            v = torch.cat((past_value, v), dim=-2)\n",
    "        \n",
    "        if use_cache:\n",
    "            present = (k, v)\n",
    "        else:\n",
    "            present = None\n",
    "\n",
    "        # Compute Q @ K^T\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
    "        attention_scores = attention_scores * self.scale\n",
    "\n",
    "        # Causal mask\n",
    "        seq_len = hidden_states.size(-2)\n",
    "        mask_value = torch.finfo(hidden_states.dtype).min\n",
    "        causal_mask = torch.triu(torch.full((seq_len, seq_len), mask_value), diagonal=1)\n",
    "        attention_scores = attention_scores + causal_mask\n",
    "\n",
    "        # Attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask[:, None, None, :]\n",
    "            attention_mask = attention_mask.to(dtype=hidden_states.dtype)\n",
    "            attention_mask = (1.0 - attention_mask) * torch.finfo(attention_mask.dtype).min\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.attn_dropout(attention_weights)\n",
    "\n",
    "        # Compute V\n",
    "        attention_scores = torch.matmul(attention_weights, v)\n",
    "\n",
    "        # Reshape\n",
    "        context_layer = attention_scores.permute(0, 2, 1, 3).contiguous()\n",
    "        context_layer = context_layer.view(batch_size, -1, self.head_size * self.n_head)\n",
    "        attention_output = self.c_proj(context_layer)\n",
    "\n",
    "        # Skip connection & Dropout\n",
    "        attention_output = self.resid_dropout(attention_output)\n",
    "\n",
    "        outputs = (attention_output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attention_weights,)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MLP(torch.nn.Module):\n",
    "    def __init__(self, inner_dim: int, config: GPT2Config) -> None:\n",
    "        super().__init__()\n",
    "        self.c_fc = Conv1D(input_dim=config.hidden_size, output_dim=inner_dim)\n",
    "        self.c_proj = Conv1D(input_dim=inner_dim, output_dim=config.hidden_size)\n",
    "        self.act = NewGELUActivation()\n",
    "        self.dropout = torch.nn.Dropout(p=config.resid_pdrop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(torch.nn.Module):\n",
    "    def __init__(self, config: GPT2Config) -> None:\n",
    "        super().__init__()\n",
    "        inner_dim = config.n_inner if config.n_inner is not None else 4 * config.hidden_size\n",
    "\n",
    "        self.ln_1 = torch.nn.LayerNorm(normalized_shape=config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.attn = GPT2Attention(config=config)\n",
    "        self.ln_2 = torch.nn.LayerNorm(normalized_shape=config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = GPT2MLP(inner_dim, config=config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        residual = hidden_states\n",
    "\n",
    "        # Self-Attention\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        attention_outputs = self.attn(\n",
    "            hidden_states=hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "\n",
    "        attention_output = attention_outputs[0]  # output_attn: (attention_output, present, all_attentions)\n",
    "        outputs = attention_outputs[1:]\n",
    "        \n",
    "        # Residual connection\n",
    "        hidden_states = attention_output + residual\n",
    "        residual = hidden_states\n",
    "\n",
    "        # MLP\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = hidden_states + residual\n",
    "\n",
    "        # Cache\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs  # outputs: (hidden_states, present, all_attentions)\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs  # outputs: (hidden_states, all_attentions)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(torch.nn.Module):\n",
    "    def __init__(self, config: GPT2Config) -> None:\n",
    "        super().__init__()\n",
    "        self.wte = torch.nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = torch.nn.Embedding(config.n_positions, config.n_embd)\n",
    "        self.dropout = torch.nn.Dropout(p=config.embd_pdrop)\n",
    "        self.h = torch.nn.ModuleList([GPT2Block(config=config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = torch.nn.LayerNorm(normalized_shape=config.n_embd, eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "    ) -> BaseModelOutputWithPastAndCrossAttentions:\n",
    "        # Token embeddings\n",
    "        token_embeddings = self.wte(input_ids)\n",
    "\n",
    "        # Position embeddings\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(0, input_ids.size(1))\n",
    "            position_embeddings = self.wpe(position_ids).view(1, -1, token_embeddings.size(-1))\n",
    "        else:\n",
    "            position_embeddings = self.wpe(position_ids)\n",
    "\n",
    "        # Sum the embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "        # KV Cache\n",
    "        if past_key_values is None:\n",
    "            past_key_values = tuple([None] * len(self.h))\n",
    "\n",
    "        # Computation\n",
    "        presents = () if use_cache else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        \n",
    "        hidden_states = self.dropout(embeddings)\n",
    "\n",
    "        for block, layer_past in zip(self.h, past_key_values):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states)\n",
    "\n",
    "            outputs = block(\n",
    "                hidden_states=hidden_states,\n",
    "                layer_past=layer_past,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "\n",
    "            hidden_states = outputs[0]\n",
    "\n",
    "            if use_cache is True:\n",
    "                presents = presents + (outputs[1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (outputs[2 if use_cache is True else 1],)\n",
    "        \n",
    "        # LayerNorm\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=presents,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyGPT2LMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGPT2LMHeadModel(torch.nn.Module):\n",
    "    def __init__(self, config: GPT2Config) -> None:\n",
    "        super().__init__()\n",
    "        # Cache\n",
    "        self.use_cache = config.use_cache\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "\n",
    "        self.transformer = GPT2Model(config=config)\n",
    "        self.lm_head = torch.nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self) -> None:\n",
    "        self.lm_head.weight = self.transformer.wte.weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "    ) -> CausalLMOutputWithCrossAttentions:\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "\n",
    "        hidden_states = transformer_outputs.last_hidden_state\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        # Loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(lm_logits.device)\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_pretrained(pretrained_model_name_or_path: str) -> \"MyGPT2LMHeadModel\":\n",
    "        \"\"\"Load pretrained weights from HuggingFace into model.\n",
    "        \n",
    "        Args:\n",
    "            pretrained_model_name_or_path: One of\n",
    "                * \"openai-community/gpt2\"\n",
    "                ...\n",
    "\n",
    "        Returns:\n",
    "            model: BertModel model with weights loaded\n",
    "        \"\"\"\n",
    "\n",
    "        def load_state_dict_hf(path_or_repo_id: str) -> OrderedDict:\n",
    "            resolved_archive_file = cached_file(\n",
    "                path_or_repo_id=path_or_repo_id,\n",
    "                filename=WEIGHTS_NAME,\n",
    "            )\n",
    "            return torch.load(resolved_archive_file, weights_only=True)\n",
    "\n",
    "        # Load config\n",
    "        config = GPT2Config.from_pretrained_model_or_path(pretrained_model_name_or_path=pretrained_model_name_or_path)\n",
    "\n",
    "        # Load weights\n",
    "        state_dict = load_state_dict_hf(pretrained_model_name_or_path)\n",
    "\n",
    "        new_state_dict = {}\n",
    "        for key in state_dict:\n",
    "            if not re.findall(r\"h.\\d+.attn.[wb]\", key):\n",
    "                new_key = \"transformer.\" + key\n",
    "                new_state_dict[new_key] = state_dict[key]\n",
    "\n",
    "        # Load model\n",
    "        model = MyGPT2LMHeadModel(config=config)\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @torch.no_grad\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        max_length: int = 20,\n",
    "        top_p: float = 0.9,\n",
    "        top_k: int = 10,\n",
    "        no_repeat_ngram_size: int = 2,\n",
    "        early_stopping: bool = False,\n",
    "        use_cache: bool = True,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "    ):\n",
    "        # Prepare input\n",
    "        batch_size = input_ids.shape[0]\n",
    "        past_key_values = None\n",
    "        generation_mode = \"greedy\"\n",
    "        finished = torch.zeros(batch_size, dtype=torch.bool)\n",
    "        all_sequences = input_ids\n",
    "        use_cache = use_cache if use_cache is not None else self.use_cache\n",
    "        print(\"use_cache:\", use_cache)\n",
    "\n",
    "        # Position ids\n",
    "        position_ids = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "        \n",
    "        # Greedy search\n",
    "        if generation_mode == \"greedy\":\n",
    "            for idx in range(max_length):\n",
    "                outputs = self(\n",
    "                    input_ids=input_ids,\n",
    "                    past_key_values=past_key_values,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    use_cache=self.use_cache,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                )\n",
    "\n",
    "                past_key_values = outputs.past_key_values if use_cache else None\n",
    "                lm_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "                # Next token\n",
    "                next_token = torch.argmax(lm_logits, dim=-1, keepdim=True)\n",
    "\n",
    "                # Determine finished\n",
    "                just_finished = next_token.squeeze(-1) == self.eos_token_id\n",
    "                finished = finished | just_finished\n",
    "\n",
    "                # Update input_ids\n",
    "                next_token = torch.where(\n",
    "                    condition=finished.unsqueeze(-1),\n",
    "                    input=torch.full_like(next_token, self.eos_token_id),\n",
    "                    other=next_token,\n",
    "                )\n",
    "                all_sequences = torch.cat([all_sequences, next_token], dim=1)\n",
    "\n",
    "                if use_cache:\n",
    "                    input_ids = next_token\n",
    "                else:\n",
    "                    input_ids = all_sequences\n",
    "\n",
    "                # Update position_ids\n",
    "                new_position_ids = position_ids[:, -1:] + 1\n",
    "                new_position_ids = torch.where(\n",
    "                    condition=finished.unsqueeze(-1),\n",
    "                    input=torch.ones_like(new_position_ids),\n",
    "                    other=new_position_ids,\n",
    "                )\n",
    "\n",
    "                if use_cache:\n",
    "                    position_ids = new_position_ids\n",
    "                else:\n",
    "                    position_ids = torch.cat([position_ids, new_position_ids], dim=1)\n",
    "\n",
    "                # Update attention_mask\n",
    "                new_attention_mask_column = torch.ones((batch_size, 1), device=input_ids.device, dtype=torch.long)\n",
    "                attention_mask = torch.cat([attention_mask, new_attention_mask_column], dim=1)\n",
    "\n",
    "                if finished.all():\n",
    "                    break\n",
    "            \n",
    "            return all_sequences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = MyGPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sentences = [\n",
    "    \"Today is a nice day\",\n",
    "    \"I want to go to play\",\n",
    "    \"Hello\",\n",
    "    \"Nice to meet you too\",\n",
    "]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    sentences,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cache: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Today is a nice day<|endoftext|>The day is bright and sunny, and the sun is shining. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon',\n",
       " 'I want to go to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you',\n",
       " 'Hello<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\nI\\'m not sure if you\\'re aware of the fact that the US government has been using the term \"global warming\" to describe the global warming that is happening in the US.\\n\\nThe US government has been using the term \"global warming\" to describe the global warming that is happening in the US.\\n\\nThe US government has been using the term \"global warming\" to describe the global warming that is happening in the US.\\n\\nThe US government has been using the term',\n",
       " \"Nice to meet you too<|endoftext|>\\n\\nI'm a little bit of a fan of the old school, but I'm not sure if I'm ready for the new.\\n\\nI'm not sure if I'm ready for the new. I'm not sure if I'm ready for the new. I'm not sure if I'm ready for the new. I'm not sure if I'm ready for the new. I'm not sure if I'm ready for the new. I'm not sure if I'm ready for the\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(custom_model.generate(**inputs, max_length=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cache: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Today is a nice day<|endoftext|>The day is bright and sunny, and the sun is shining. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon. The sun is shining on the horizon',\n",
       " 'I want to go to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you. I want to play with you',\n",
       " 'Hello<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\nI\\'m not sure if you\\'re aware of the fact that the US government has been using the term \"global warming\" to describe the global warming that is happening in the US.\\n\\nThe US government has been using the term \"global warming\" to describe the global warming that is happening in the US.\\n\\nThe US government has been using the term \"global warming\" to describe the global warming that is happening in the US.\\n\\nThe US government has been using the term',\n",
       " \"Nice to meet you too<|endoftext|>\\n\\nI'm a little bit of a fan of the old school, but I'm not sure if I'm ready for the new.\\n\\nI'm not sure if I'm ready for the new. I'm not sure if I'm ready for the new. I'm not sure if I'm ready for the new. I'm not sure if I'm ready for the new. I'm not sure if I'm ready for the new. I'm not sure if I'm ready for the\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(custom_model.generate(**inputs, max_length=100, use_cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(torch.Size([4, 1, 768])) + torch.rand(torch.Size([4, 6, 768]))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -36.3291,  -36.3401,  -40.4227,  ...,  -46.0233,  -44.5283,\n",
       "           -37.1275],\n",
       "         [-122.8355, -122.5402, -127.6362,  ..., -133.4906, -131.9769,\n",
       "          -125.4614],\n",
       "         [-114.4519, -113.7912, -117.0852,  ..., -124.7081, -120.6112,\n",
       "          -114.5219],\n",
       "         [-109.6922, -108.5661, -114.7928,  ..., -119.8786, -118.4212,\n",
       "          -111.4149],\n",
       "         [ -88.5250,  -90.9243,  -98.3161,  ..., -105.2264, -101.8231,\n",
       "           -94.2055],\n",
       "         [ -88.6964,  -81.4597,  -84.0843,  ..., -101.0075, -101.5497,\n",
       "           -88.3269]],\n",
       "\n",
       "        [[ -39.3084,  -39.0100,  -41.8375,  ...,  -46.9338,  -44.9074,\n",
       "           -39.5149],\n",
       "         [ -68.6073,  -68.2686,  -74.1494,  ...,  -76.0180,  -78.3470,\n",
       "           -72.2143],\n",
       "         [-137.8777, -137.4907, -143.7934,  ..., -147.5330, -148.3120,\n",
       "          -140.3079],\n",
       "         [-107.0322, -107.4709, -113.6475,  ..., -116.8958, -117.8132,\n",
       "          -111.0006],\n",
       "         [ -80.3674,  -79.9714,  -84.8500,  ...,  -86.0617,  -89.0680,\n",
       "           -82.1314],\n",
       "         [ -96.4428,  -97.2568, -104.0048,  ..., -106.5366, -109.0130,\n",
       "          -100.5894]],\n",
       "\n",
       "        [[ -35.2362,  -35.3265,  -38.9753,  ...,  -44.4644,  -43.9974,\n",
       "           -36.4579],\n",
       "         [ -70.3748,  -66.9890,  -70.8384,  ...,  -78.9746,  -78.0599,\n",
       "           -68.2919],\n",
       "         [ -70.9211,  -67.1661,  -71.0032,  ...,  -80.0009,  -79.1074,\n",
       "           -68.6626],\n",
       "         [ -71.9456,  -68.0350,  -71.8257,  ...,  -81.2031,  -80.3837,\n",
       "           -69.6212],\n",
       "         [ -72.7737,  -68.7415,  -72.5372,  ...,  -82.1493,  -81.4141,\n",
       "           -70.4422],\n",
       "         [ -73.3893,  -69.2481,  -73.0412,  ...,  -82.8917,  -82.2111,\n",
       "           -71.0389]],\n",
       "\n",
       "        [[ -29.8480,  -29.6480,  -33.3091,  ...,  -38.2261,  -37.5072,\n",
       "           -30.5415],\n",
       "         [-137.1740, -136.7039, -141.9966,  ..., -143.3851, -143.1121,\n",
       "          -136.9452],\n",
       "         [ -93.2310,  -94.9168,  -99.6967,  ..., -102.2250, -102.9242,\n",
       "           -97.1929],\n",
       "         [ -95.5427,  -97.1026, -103.2693,  ..., -108.8357, -108.4238,\n",
       "          -100.5663],\n",
       "         [ -72.9992,  -74.4645,  -81.9949,  ...,  -87.2422,  -86.9052,\n",
       "           -78.3751],\n",
       "         [ -98.2934,  -91.3982,  -94.2750,  ..., -111.6120, -112.0108,\n",
       "           -96.0971]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/opt/conda3/envs/llms'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.benchmark as benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "sequence_length = 64\n",
    "hidden_size = 512\n",
    "num_heads = 8\n",
    "input_1 = torch.randn(batch_size, sequence_length, hidden_size)\n",
    "input_2 = torch.randn(batch_size, sequence_length, hidden_size)\n",
    "input_3 = torch.randn(batch_size, sequence_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attention = torch.nn.MultiheadAttention(hidden_size, num_heads)\n",
    "timer = benchmark.Timer(\n",
    "    stmt=\"multihead_attention(input_1, input_2, input_3)\",\n",
    "    setup=\"import torch\",\n",
    "    globals={\"multihead_attention\": multihead_attention, \"input_1\": input_1, \"input_2\": input_2, \"input_3\": input_3},\n",
    "    num_threads=4)\n",
    "multihead_attention_time = timer.timeit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GPT2Config at 0x7f7ebaca1db0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.hidden_size = hidden_size\n",
    "config.n_head = num_heads\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_multihead_attention = MyAttention(config=config)\n",
    "timer = benchmark.Timer(\n",
    "    stmt=\"my_multihead_attention(input_1, input_2, input_3)\",\n",
    "    setup=\"import torch\",\n",
    "    globals={\"my_multihead_attention\": my_multihead_attention, \"input_1\": input_1, \"input_2\": input_2, \"input_3\": input_3},\n",
    "    num_threads=4)\n",
    "multihead_attention_time = timer.timeit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
